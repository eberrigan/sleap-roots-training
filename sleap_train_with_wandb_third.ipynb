{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import wandb\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs\n",
    "# ! pip install ipywidgets\n",
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder to login to wandb if you haven't already\n",
    "# ! wandb login\n",
    "# ! wandb status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to working directory\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/sorghum_soybean\" # This should be the same as the previous notebook\n",
    "working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/sorghum\" # This should be the same as the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: \\home\\jovyan\\work\\20250102_generalizability_experiment\\primary\\sorghum\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory\n",
    "cwd = Path(working_dir)\n",
    "print(f\"Current working directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for W&B initialization\n",
    "ENTITY_NAME = \"eberrigan-salk-institute-for-biological-studies\"\n",
    "PROJECT_NAME = \"sleap-roots\"\n",
    "EXPERIMENT_NAME = \"sorghum-primary-2025-01-06\"  # Use a unique name for this experiment\n",
    "JOB_TYPE = \"train\"\n",
    "\n",
    "# To build sleap-train command for each split/W&B run\n",
    "CSV_PATH = cwd / \"train_test_splits.csv\"\n",
    "SLEAP_TRAIN_COMMAND = \"sleap-train {}\"\n",
    "USE_EXISTING_MODEL = False\n",
    "\n",
    "# Tags for the W&B run\n",
    "# TAGS = [\"soybean6-8DAG\", \"sorghum5-12DAG\", \"soybean\", \"sorghum\", \"primary\", \"2025-01-06\"]\n",
    "TAGS = [\"sorghum5-12DAG\", \"sorghum\", \"primary\", \"2025-01-06\", \"5-12DAG\"]\n",
    "\n",
    "# Tags for the model artifact in W&B\n",
    "# MODEL_TAGS = [\"soybean6-8DAG\", \"sorghum5-12DAG\", \"soybean\", \"sorghum\" \"primary\", \"2025-01-06\"]\n",
    "MODEL_TAGS = [\"sorghum5-12DAG\", \"sorghum\", \"primary\", \"2025-01-06\", \"5-12DAG\"]\n",
    "\n",
    "# Wandb notebook name for code saving\n",
    "WANDB_NOTEBOOK_NAME = \"sleap_train_with_wandb_third.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the W&B environment variables\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = WANDB_NOTEBOOK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(csv_path):\n",
    "    \"\"\"Loads training data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (Path): Path to the CSV file containing training data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the training data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "def get_training_groups(df):\n",
    "    \"\"\"Groups training data by version.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the training data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.core.groupby.DataFrameGroupBy: Grouped DataFrame.\n",
    "    \"\"\"\n",
    "    return df.groupby(\"version\")\n",
    "\n",
    "\n",
    "def log_to_wandb(project_name, entity_name, experiment_name, version, config, config_path, tags=None):\n",
    "    \"\"\"Initializes a W&B run and logs the initial training configuration.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): Name of the W&B project--group of experiments.\n",
    "        entity_name (str): Name of the W&B entity--organization or user.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        version (str): Version of the training run.\n",
    "        config (dict): Configuration dictionary loaded from the JSON file.\n",
    "        config_path (Path): Path to the training configuration file.\n",
    "        tags (list, optional): List of tags to be added to the W&B run.\n",
    "\n",
    "    Returns:\n",
    "        wandb.Run: W&B run object.\n",
    "    \"\"\"\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        entity=entity_name,\n",
    "        group=experiment_name,\n",
    "        config=config,\n",
    "        name=f\"{experiment_name}_training_v00{version}\", # Unique name for the run\n",
    "        tags=tags,\n",
    "        mode=\"online\",  # default\n",
    "    )\n",
    "    # Log the version and path to the config\n",
    "    wandb.config.update({\"version\": version, \"config_path\": config_path.as_posix()})\n",
    "    return run\n",
    "\n",
    "\n",
    "def execute_training(command):\n",
    "    \"\"\"Executes the training command using subprocess.\n",
    "\n",
    "    Args:\n",
    "        command (str): Training command to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Executing: {command}\")\n",
    "    try:\n",
    "        # Run the command in a subprocess\n",
    "        result = subprocess.run(\n",
    "            command, \n",
    "            shell=True,           # Run the command through the shell\n",
    "            check=True,           # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,  # Capture standard output\n",
    "            stderr=subprocess.PIPE,  # Capture standard error\n",
    "            text=True             # Decode output as text (not bytes)\n",
    "        )\n",
    "        # Print real-time output to monitor training progress\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle errors by printing the stderr\n",
    "        print(f\"Error executing training command: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def log_model_artifact(run, experiment_name, model_tags, model_dir, version):\n",
    "    \"\"\"Logs a trained model as a W&B artifact and updates the W&B run config with the training configuration.\n",
    "\n",
    "    Args:\n",
    "        run (wandb.Run): The W&B run object.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        model_tags (list): List of tags to be added to the model artifact.\n",
    "        model_dir (Path): Path to the directory containing the trained model.\n",
    "        version (str): Version of the training run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Path to the training config\n",
    "    training_config_path = model_dir / \"training_config.json\"\n",
    "    training_config = {}\n",
    "\n",
    "    # Load the training configuration if it exists\n",
    "    if training_config_path.exists():\n",
    "        with open(training_config_path, \"r\") as f:\n",
    "            training_config = json.load(f)\n",
    "\n",
    "        # Update the W&B run configuration\n",
    "        run.config.update(training_config)\n",
    "        print(\"W&B run configuration updated with training configuration.\")\n",
    "\n",
    "    # Create artifact\n",
    "    # https://docs.wandb.ai/ref/python/artifact/\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"{experiment_name}_v00{version}\",  # Unique name for the artifact\n",
    "        type=\"model\",\n",
    "        metadata={\n",
    "            \"experiment\": experiment_name,\n",
    "            \"version\": version,\n",
    "            **training_config,  # Add training config metadata if available\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add tags to the artifact\n",
    "    for tag in model_tags:\n",
    "        # Add tags as metadata\n",
    "        model_artifact.metadata[tag] = True\n",
    "    # tags are an attribute of the artifact object\n",
    "    model_artifact.tags = model_tags\n",
    "\n",
    "    # Add the entire model directory to the artifact\n",
    "    model_artifact.add_dir(model_dir)\n",
    "\n",
    "    # Log the artifact to the W&B run\n",
    "    run.log_artifact(model_artifact)\n",
    "    print(f\"Model artifact '{model_artifact.name}' logged to W&B.\")\n",
    "\n",
    "\n",
    "def evaluate_model_and_generate_visuals(model_dir, px_per_mm=17.0):\n",
    "    \"\"\"Evaluates the model and generates visualizations for metrics.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str or Path): Path to the directory containing the trained model.\n",
    "        px_per_mm (float): Pixel scaling factor for converting distances to mm.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            metrics_summary_df (pd.DataFrame): DataFrame containing summary metrics.\n",
    "            dists_df (pd.DataFrame): DataFrame containing detailed distances.\n",
    "            visualizations (dict): Dictionary of visualization names and file paths.\n",
    "    \"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    if not model_dir.exists():\n",
    "        raise FileNotFoundError(f\"Model directory not found at: {model_dir}\")\n",
    "    \n",
    "    model_dir_str = model_dir.as_posix()\n",
    "    print(f\"Model path: {model_dir_str}\")\n",
    "\n",
    "    # Load the model\n",
    "    metrics = sleap.load_metrics(model_dir_str, split=\"test\")\n",
    "    print(f\"Metrics loaded from model directory: {model_dir_str}\")\n",
    "\n",
    "    # Extract summary metrics\n",
    "    metrics_summary = {\n",
    "        \"model_path\": model_dir_str,\n",
    "        \"model_name\":model_dir.name,\n",
    "        \"dist_p50\": metrics[\"dist.p50\"] / px_per_mm,\n",
    "        \"dist_p90\": metrics[\"dist.p90\"] / px_per_mm,\n",
    "        \"dist_p95\": metrics[\"dist.p95\"] / px_per_mm,\n",
    "        \"dist_p99\": metrics[\"dist.p99\"] / px_per_mm,\n",
    "        \"dist_avg\": metrics[\"dist.avg\"] / px_per_mm,\n",
    "        \"dist_std\": np.nanstd(metrics[\"dist.dists\"].flatten()) / px_per_mm,\n",
    "        \"vis_prec\": metrics[\"vis.precision\"],\n",
    "        \"vis_recall\": metrics[\"vis.recall\"],\n",
    "        \"oks_map\": metrics[\"oks_voc.mAP\"],\n",
    "        \"oks_mar\": metrics[\"oks_voc.mAR\"]\n",
    "    }\n",
    "\n",
    "    metrics_summary_df = pd.DataFrame([metrics_summary])\n",
    "\n",
    "    # Save detailed distance metrics\n",
    "    dists = metrics[\"dist.dists\"].flatten() / px_per_mm\n",
    "    dists_df = pd.DataFrame({\"distances_mm\": dists})\n",
    "\n",
    "    # Generate histogram for distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(dists, bins=30, kde=True, color=\"blue\")\n",
    "    plt.axvline(metrics_summary[\"dist_p50\"], color=\"green\", linestyle=\"--\", label=\"50th Percentile\")\n",
    "    plt.axvline(metrics_summary[\"dist_p90\"], color=\"orange\", linestyle=\"--\", label=\"90th Percentile\")\n",
    "    plt.axvline(metrics_summary[\"dist_avg\"], color=\"red\", linestyle=\"--\", label=\"Average Distance\")\n",
    "    plt.title(\"Distribution of Distances\")\n",
    "    plt.xlabel(\"Distance (mm)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    histogram_path = model_dir / \"distance_histogram.png\"\n",
    "    plt.savefig(histogram_path)\n",
    "    plt.close()\n",
    "\n",
    "    visualizations = {\"distance_histogram\": histogram_path}\n",
    "\n",
    "    return metrics_summary_df, dists_df, visualizations\n",
    "\n",
    "\n",
    "def log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, eval_fn, eval_args):\n",
    "    \"\"\"Logs a trained model as a W&B artifact, updates the W&B run config with the training configuration,\n",
    "    and logs evaluation metrics and visualizations.\n",
    "\n",
    "    Args:\n",
    "        run (wandb.Run): The W&B run object.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        model_tags (list): List of tags to be added to the model artifact.\n",
    "        model_dir (Path): Path to the directory containing the trained model.\n",
    "        version (str): Version of the training run.\n",
    "        eval_fn (callable): Function to evaluate the model.\n",
    "        eval_args (dict): Arguments required for the evaluation function.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Path to the training config\n",
    "    training_config_path = model_dir / \"training_config.json\"\n",
    "    training_config = {}\n",
    "\n",
    "    # Load the training configuration if it exists\n",
    "    if training_config_path.exists():\n",
    "        with open(training_config_path, \"r\") as f:\n",
    "            training_config = json.load(f)\n",
    "\n",
    "        # Update the W&B run configuration\n",
    "        run.config.update(training_config)\n",
    "        print(\"W&B run configuration updated with training configuration.\")\n",
    "\n",
    "    # Create artifact\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"{experiment_name}_v00{version}\",  # Unique name for the artifact\n",
    "        type=\"model\",\n",
    "        metadata={\n",
    "            \"experiment\": experiment_name,\n",
    "            \"version\": version,\n",
    "            **training_config,  # Add training config metadata if available\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add tags to the artifact\n",
    "    for tag in model_tags:\n",
    "        model_artifact.metadata[tag] = True\n",
    "\n",
    "    # Add the entire model directory to the artifact\n",
    "    model_artifact.add_dir(model_dir)\n",
    "\n",
    "    # Perform model evaluation\n",
    "    metrics_summary_df, dists_df, visualizations = eval_fn(**eval_args)\n",
    "\n",
    "    # Save evaluation metrics as artifacts\n",
    "    metrics_summary_csv_path = model_dir / \"metrics_summary.csv\"\n",
    "    metrics_summary_df.to_csv(metrics_summary_csv_path, index=False)\n",
    "    model_artifact.add_file(metrics_summary_csv_path)\n",
    "\n",
    "    dists_csv_path = model_dir / \"detailed_distances.csv\"\n",
    "    dists_df.to_csv(dists_csv_path, index=False)\n",
    "    model_artifact.add_file(dists_csv_path)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    for metric_name, metric_value in metrics_summary_df.iloc[0].items():\n",
    "        run.summary[metric_name] = metric_value\n",
    "        model_artifact.metadata[metric_name] = metric_value\n",
    "\n",
    "    # Log visualizations\n",
    "    for viz_name, viz_path in visualizations.items():\n",
    "        model_artifact.add_file(viz_path)\n",
    "\n",
    "    # Log the artifact to the W&B run\n",
    "    run.log_artifact(model_artifact, type=\"model\", tags=model_tags)\n",
    "    print(f\"Model artifact '{model_artifact.name}' logged to W&B with evaluations.\")\n",
    "\n",
    "\n",
    "def process_training(project_name, entity_name, experiment_name, version, group, use_existing_model, sleap_train_command, tags=None, model_tags=None):\n",
    "    \"\"\"Processes a training run for a specific version.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): Name of the W&B project--group of experiments.\n",
    "        entity_name (str): Name of the W&B entity--organization or user.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        version (str): Version of the training run.\n",
    "        group (pandas.DataFrame): Group of rows corresponding to the version.\n",
    "        use_existing_model (bool): Whether to use an existing model for evaluation.\n",
    "        sleap_train_command (str): Training command to be executed.\n",
    "        tags (list, optional): List of tags to be added to the W&B run.\n",
    "        model_tags (list, optional): List of tags to be added to the model artifact.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dir_path = Path(group.iloc[0][\"path\"]).parent\n",
    "    print(f\"Directory path for version {version}: {dir_path}\")\n",
    "\n",
    "    config_path = dir_path / f\"initial_config_modified_v00{version}.json\"\n",
    "\n",
    "    if config_path.exists():\n",
    "        # Load the training configuration\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Start W&B run\n",
    "        run = log_to_wandb(\n",
    "            project_name=project_name,\n",
    "            entity_name=entity_name,\n",
    "            experiment_name=experiment_name,\n",
    "            version=version,\n",
    "            config=config,\n",
    "            config_path=config_path,\n",
    "            tags=tags\n",
    "        )\n",
    "        if use_existing_model:\n",
    "            # Check if the model directory exists\n",
    "            model_dir = dir_path / \"models\"\n",
    "            if model_dir.exists() and model_dir.is_dir():\n",
    "                # Get all subdirectories in the models directory\n",
    "                subdirectories = [subdir for subdir in model_dir.iterdir() if subdir.is_dir()]\n",
    "\n",
    "                if len(subdirectories) == 1:\n",
    "                    # If exactly one subdirectory exists, get its name\n",
    "                    subdirectory_name = subdirectories[0].name\n",
    "                    print(f\"Subdirectory name: {subdirectory_name}\")\n",
    "\n",
    "                    # Construct the path to the model directory for this run\n",
    "                    model_dir = model_dir / subdirectory_name\n",
    "                    print(f\"Model directory path: {model_dir}\")\n",
    "\n",
    "                    # Log the model as a W&B artifact with evaluation metrics and visualizations\n",
    "                    log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, evaluate_model_and_generate_visuals, {\"model_dir\": model_dir, \"px_per_mm\": 17.0})\n",
    "\n",
    "                elif len(subdirectories) == 0:\n",
    "                    raise FileNotFoundError(f\"No subdirectories found in the models directory for version {version}: {model_dir}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"More than one subdirectory found in the models directory for version {version}: {subdirectories}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Models directory does not exist for version {version}: {model_dir}\")\n",
    "        else:\n",
    "            try:\n",
    "                # Execute training command\n",
    "                command = sleap_train_command.format(config_path.as_posix())\n",
    "                # Debugging: Print the command to verify it is correct\n",
    "                print(f\"Prepared training command: {command}\")\n",
    "                \n",
    "                execute_training(command)\n",
    "\n",
    "                model_dir = dir_path / \"models\"\n",
    "\n",
    "                # Ensure the models directory exists\n",
    "                if model_dir.exists() and model_dir.is_dir():\n",
    "                    # Get all subdirectories in the models directory\n",
    "                    subdirectories = [subdir for subdir in model_dir.iterdir() if subdir.is_dir()]\n",
    "\n",
    "                    if len(subdirectories) == 1:\n",
    "                        # If exactly one subdirectory exists, get its name\n",
    "                        subdirectory_name = subdirectories[0].name\n",
    "                        print(f\"Subdirectory name: {subdirectory_name}\")\n",
    "\n",
    "                        # Construct the path to the model directory for this run\n",
    "                        model_dir = model_dir / subdirectory_name\n",
    "                        print(f\"Model directory path: {model_dir}\")\n",
    "\n",
    "                        # Log the model with evaluation metrics and visualizations as a W&B artifact\n",
    "                        log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, evaluate_model_and_generate_visuals, {\"model_dir\": model_dir, \"px_per_mm\": 17.0})\n",
    "\n",
    "                    elif len(subdirectories) == 0:\n",
    "                        raise FileNotFoundError(f\"No subdirectories found in the models directory for version {version}: {model_dir}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"More than one subdirectory found in the models directory for version {version}: {subdirectories}\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Models directory does not exist for version {version}: {model_dir}\")\n",
    "\n",
    "            finally:\n",
    "                # Ensure the W&B run is finished\n",
    "                run.finish()\n",
    "                print(f\"W&B run for version {version} finished.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Config file not found for version {version}: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(csv_path, use_existing_model=False):\n",
    "    \"\"\"Main function to process all training runs.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (Path): Path to the CSV file containing train-test splits paths.\n",
    "        use_existing_model (bool, optional): Whether to use an existing model for evaluation.\n",
    "    \"\"\"\n",
    "    df = load_training_data(csv_path)\n",
    "    grouped = get_training_groups(df)\n",
    "\n",
    "    for version, group in grouped:\n",
    "        print(f\"Processing version {version}...\")\n",
    "        print(f\"Group: {group}\")\n",
    "        process_training(\n",
    "            project_name=PROJECT_NAME,\n",
    "            entity_name=ENTITY_NAME,\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            version=version,\n",
    "            group=group,\n",
    "            use_existing_model=use_existing_model,\n",
    "            sleap_train_command=SLEAP_TRAIN_COMMAND,\n",
    "            tags=TAGS,\n",
    "            model_tags=MODEL_TAGS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\home\\\\jovyan\\\\work\\\\20250102_generalizability_experiment\\\\primary\\\\sorghum\\\\train_test_splits.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14644\\3672302118.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run the main function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_existing_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUSE_EXISTING_MODEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14644\\2752713110.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(csv_path, use_existing_model)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0muse_existing_model\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mWhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0muse\u001b[0m \u001b[0man\u001b[0m \u001b[0mexisting\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mgrouped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_training_groups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14644\\241808355.py\u001b[0m in \u001b[0;36mload_training_data\u001b[1;34m(csv_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\pbiobgh\\.conda\\envs\\sleap_v1.3.4\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\home\\\\jovyan\\\\work\\\\20250102_generalizability_experiment\\\\primary\\\\sorghum\\\\train_test_splits.csv'"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "main(CSV_PATH, use_existing_model=USE_EXISTING_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap_v1.3.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
