{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import wandb\n",
    "import subprocess\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sleap\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip installs\n",
    "# ! pip install ipywidgets\n",
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder to login to wandb if you haven't already\n",
    "# ! wandb login\n",
    "# ! wandb status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to working directory\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/sorghum_soybean\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/sorghum\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/sorghum\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/soybean\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/sorghum_soybean\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/canola\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/pennycress\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/arabidopsis\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/arabidopsis\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/pennycress\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/canola\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/younger_rice\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/sorghum_soybean_canola_pennycress_rice_arabidopsis\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/primary/canola_pennycress_arabidopsis\" # This should be the same as the previous notebook\n",
    "# working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/pennycress\" # This should be the same as the previous notebook\n",
    "working_dir = \"D:/SLEAP/20250102_generalizability_experiment/lateral/canola\" # This should be the same as the previous notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\SLEAP\\20250102_generalizability_experiment\\lateral\\canola\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory\n",
    "cwd = Path(working_dir)\n",
    "print(f\"Current working directory: {cwd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for W&B initialization\n",
    "ENTITY_NAME = \"eberrigan-salk-institute-for-biological-studies\"\n",
    "PROJECT_NAME = \"sleap-roots\"\n",
    "EXPERIMENT_NAME = \"canola-lateral-2025-01-21\"  # Use a unique name for this experiment\n",
    "JOB_TYPE = \"train\"\n",
    "\n",
    "# To build sleap-train command for each split/W&B run\n",
    "CSV_PATH = cwd / \"train_test_splits.csv\"\n",
    "SLEAP_TRAIN_COMMAND = \"sleap-train {}\"\n",
    "USE_EXISTING_MODEL = False\n",
    "\n",
    "# Tags for the W&B run\n",
    "# TAGS = [\"soybean6-8DAG\", \"sorghum5-12DAG\", \"soybean\", \"sorghum\", \"primary\", \"2025-01-07\"]\n",
    "# TAGS = [\"sorghum5-12DAG\", \"sorghum\", \"primary\", \"2025-01-06\", \"5-12DAG\"]\n",
    "# TAGS = [\"soybean6DAG\", \"sorghum5-10DAG\", \"soybean\", \"sorghum\", \"lateral\", \"2025-01-10\"]\n",
    "# TAGS = [\"canola2-13DAG\", \"2-13DAG\", \"primary\", \"2025-01-15\"]\n",
    "# TAGS = [\"rice3-5DAG\", \"soybean6-8DAG\", \"sorghum5-12DAG\", \"soybean\", \"sorghum\", \"canola2-13DAG\", \"canola\", \"pennycress\", \"arabidopsis\", \"pennycress14DAG\", \"arabidopsis7-11DAG\", \"primary\", \"2025-01-19\", \"rice\"]\n",
    "# TAGS = [\"canola2-13DAG\", \"canola\", \"pennycress\", \"arabidopsis\", \"pennycress14DAG\", \"arabidopsis7-11DAG\", \"primary\", \"2025-01-20\"]\n",
    "TAGS = [\"canola\", \"lateral\", \"2025-01-21\", \"2-13DAG\", \"canola2-13DAG\"]\n",
    "\n",
    "# Tags for the model artifact in W&B\n",
    "# MODEL_TAGS = [\"soybean6-8DAG\", \"sorghum5-12DAG\", \"soybean\", \"sorghum\", \"primary\", \"2025-01-07\"]\n",
    "# MODEL_TAGS = [\"sorghum5-12DAG\", \"sorghum\", \"primary\", \"2025-01-06\", \"5-12DAG\"]\n",
    "# MODEL_TAGS = [\"soybean6DAG\", \"sorghum5-10DAG\", \"soybean\", \"sorghum\", \"lateral\", \"2025-01-10\"]\n",
    "# MODEL_TAGS = [\"canola2-13DAG\", \"2-13DAG\", \"primary\", \"2025-01-15\"]\n",
    "MODEL_TAGS = [\"canola\", \"lateral\", \"2025-01-21\", \"2-13DAG\", \"canola2-13DAG\"]\n",
    "# Wandb notebook name for code saving\n",
    "WANDB_NOTEBOOK_NAME = \"sleap_train_with_wandb_third.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the W&B environment variables\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = WANDB_NOTEBOOK_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(csv_path):\n",
    "    \"\"\"Loads training data from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_path (Path): Path to the CSV file containing training data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the training data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "def get_training_groups(df):\n",
    "    \"\"\"Groups training data by version.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame containing the training data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.core.groupby.DataFrameGroupBy: Grouped DataFrame.\n",
    "    \"\"\"\n",
    "    return df.groupby(\"version\")\n",
    "\n",
    "\n",
    "def log_to_wandb(project_name, entity_name, experiment_name, version, config, config_path, tags=None):\n",
    "    \"\"\"Initializes a W&B run and logs the initial training configuration.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): Name of the W&B project--group of experiments.\n",
    "        entity_name (str): Name of the W&B entity--organization or user.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        version (str): Version of the training run.\n",
    "        config (dict): Configuration dictionary loaded from the JSON file.\n",
    "        config_path (Path): Path to the training configuration file.\n",
    "        tags (list, optional): List of tags to be added to the W&B run.\n",
    "\n",
    "    Returns:\n",
    "        wandb.Run: W&B run object.\n",
    "    \"\"\"\n",
    "    run = wandb.init(\n",
    "        project=project_name,\n",
    "        entity=entity_name,\n",
    "        group=experiment_name,\n",
    "        config=config,\n",
    "        name=f\"{experiment_name}_training_v00{version}\", # Unique name for the run\n",
    "        tags=tags,\n",
    "        mode=\"online\",  # default\n",
    "    )\n",
    "    # Log the version and path to the config\n",
    "    wandb.config.update({\"version\": version, \"config_path\": config_path.as_posix()})\n",
    "    return run\n",
    "\n",
    "\n",
    "def execute_training(command):\n",
    "    \"\"\"Executes the training command using subprocess.\n",
    "\n",
    "    Args:\n",
    "        command (str): Training command to be executed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Executing: {command}\")\n",
    "    try:\n",
    "        # Run the command in a subprocess\n",
    "        result = subprocess.run(\n",
    "            command, \n",
    "            shell=True,           # Run the command through the shell\n",
    "            check=True,           # Raise an exception if the command fails\n",
    "            stdout=subprocess.PIPE,  # Capture standard output\n",
    "            stderr=subprocess.PIPE,  # Capture standard error\n",
    "            text=True             # Decode output as text (not bytes)\n",
    "        )\n",
    "        # Print real-time output to monitor training progress\n",
    "        print(result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Handle errors by printing the stderr\n",
    "        print(f\"Error executing training command: {e.stderr}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def log_model_artifact(run, experiment_name, model_tags, model_dir, version):\n",
    "    \"\"\"Logs a trained model as a W&B artifact and updates the W&B run config with the training configuration.\n",
    "\n",
    "    Args:\n",
    "        run (wandb.Run): The W&B run object.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        model_tags (list): List of tags to be added to the model artifact.\n",
    "        model_dir (Path): Path to the directory containing the trained model.\n",
    "        version (str): Version of the training run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Path to the training config\n",
    "    training_config_path = model_dir / \"training_config.json\"\n",
    "    training_config = {}\n",
    "\n",
    "    # Load the training configuration if it exists\n",
    "    if training_config_path.exists():\n",
    "        with open(training_config_path, \"r\") as f:\n",
    "            training_config = json.load(f)\n",
    "\n",
    "        # Update the W&B run configuration\n",
    "        run.config.update(training_config)\n",
    "        print(\"W&B run configuration updated with training configuration.\")\n",
    "\n",
    "    # Create artifact\n",
    "    # https://docs.wandb.ai/ref/python/artifact/\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"{experiment_name}_v00{version}\",  # Unique name for the artifact\n",
    "        type=\"model\",\n",
    "        metadata={\n",
    "            \"experiment\": experiment_name,\n",
    "            \"version\": version,\n",
    "            **training_config,  # Add training config metadata if available\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add tags to the artifact\n",
    "    for tag in model_tags:\n",
    "        # Add tags as metadata\n",
    "        model_artifact.metadata[tag] = True\n",
    "\n",
    "    # Add the entire model directory to the artifact\n",
    "    model_artifact.add_dir(model_dir)\n",
    "\n",
    "    # Log the artifact to the W&B run\n",
    "    run.log_artifact(model_artifact, type=\"model\", tags=model_tags)\n",
    "    print(f\"Model artifact '{model_artifact.name}' logged to W&B.\")\n",
    "\n",
    "\n",
    "def evaluate_model_and_generate_visuals(model_dir, px_per_mm=17.0):\n",
    "    \"\"\"Evaluates the model and generates visualizations for metrics.\n",
    "\n",
    "    Args:\n",
    "        model_dir (str or Path): Path to the directory containing the trained model.\n",
    "        px_per_mm (float): Pixel scaling factor for converting distances to mm.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            metrics_summary_df (pd.DataFrame): DataFrame containing summary metrics.\n",
    "            dists_df (pd.DataFrame): DataFrame containing detailed distances.\n",
    "            visualizations (dict): Dictionary of visualization names and file paths.\n",
    "    \"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    if not model_dir.exists():\n",
    "        raise FileNotFoundError(f\"Model directory not found at: {model_dir}\")\n",
    "    \n",
    "    model_dir_str = model_dir.as_posix()\n",
    "    print(f\"Model path: {model_dir_str}\")\n",
    "\n",
    "    # Load the model\n",
    "    metrics = sleap.load_metrics(model_dir_str, split=\"test\")\n",
    "    print(f\"Metrics loaded from model directory: {model_dir_str}\")\n",
    "\n",
    "    # Extract summary metrics\n",
    "    metrics_summary = {\n",
    "        \"model_path\": model_dir_str,\n",
    "        \"model_name\":model_dir.name,\n",
    "        \"dist_p50\": metrics[\"dist.p50\"] / px_per_mm,\n",
    "        \"dist_p90\": metrics[\"dist.p90\"] / px_per_mm,\n",
    "        \"dist_p95\": metrics[\"dist.p95\"] / px_per_mm,\n",
    "        \"dist_p99\": metrics[\"dist.p99\"] / px_per_mm,\n",
    "        \"dist_avg\": metrics[\"dist.avg\"] / px_per_mm,\n",
    "        \"dist_std\": np.nanstd(metrics[\"dist.dists\"].flatten()) / px_per_mm,\n",
    "        \"vis_prec\": metrics[\"vis.precision\"],\n",
    "        \"vis_recall\": metrics[\"vis.recall\"],\n",
    "        \"oks_map\": metrics[\"oks_voc.mAP\"],\n",
    "        \"oks_mar\": metrics[\"oks_voc.mAR\"]\n",
    "    }\n",
    "\n",
    "    metrics_summary_df = pd.DataFrame([metrics_summary])\n",
    "\n",
    "    # Save detailed distance metrics\n",
    "    dists = metrics[\"dist.dists\"].flatten() / px_per_mm\n",
    "    dists_df = pd.DataFrame({\"distances_mm\": dists})\n",
    "\n",
    "    # Generate histogram for distances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(dists, bins=30, kde=True, color=\"blue\")\n",
    "    plt.axvline(metrics_summary[\"dist_p50\"], color=\"green\", linestyle=\"--\", label=\"50th Percentile\")\n",
    "    plt.axvline(metrics_summary[\"dist_p90\"], color=\"orange\", linestyle=\"--\", label=\"90th Percentile\")\n",
    "    plt.axvline(metrics_summary[\"dist_avg\"], color=\"red\", linestyle=\"--\", label=\"Average Distance\")\n",
    "    plt.title(\"Distribution of Distances\")\n",
    "    plt.xlabel(\"Distance (mm)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    histogram_path = model_dir / \"distance_histogram.png\"\n",
    "    plt.savefig(histogram_path)\n",
    "    plt.close()\n",
    "\n",
    "    visualizations = {\"distance_histogram\": histogram_path}\n",
    "\n",
    "    return metrics_summary_df, dists_df, visualizations\n",
    "\n",
    "\n",
    "def log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, eval_fn, eval_args):\n",
    "    \"\"\"Logs a trained model as a W&B artifact, updates the W&B run config with the training configuration,\n",
    "    and logs evaluation metrics and visualizations.\n",
    "\n",
    "    Args:\n",
    "        run (wandb.Run): The W&B run object.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        model_tags (list): List of tags to be added to the model artifact.\n",
    "        model_dir (Path): Path to the directory containing the trained model.\n",
    "        version (str): Version of the training run.\n",
    "        eval_fn (callable): Function to evaluate the model.\n",
    "        eval_args (dict): Arguments required for the evaluation function.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Path to the training config\n",
    "    training_config_path = model_dir / \"training_config.json\"\n",
    "    training_config = {}\n",
    "\n",
    "    # Load the training configuration if it exists\n",
    "    if training_config_path.exists():\n",
    "        with open(training_config_path, \"r\") as f:\n",
    "            training_config = json.load(f)\n",
    "\n",
    "        # Update the W&B run configuration\n",
    "        run.config.update(training_config)\n",
    "        print(\"W&B run configuration updated with training configuration.\")\n",
    "\n",
    "    # Create artifact\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=f\"{experiment_name}_v00{version}\",  # Unique name for the artifact\n",
    "        type=\"model\",\n",
    "        metadata={\n",
    "            \"experiment\": experiment_name,\n",
    "            \"version\": version,\n",
    "            **training_config,  # Add training config metadata if available\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add tags to the artifact\n",
    "    for tag in model_tags:\n",
    "        model_artifact.metadata[tag] = True\n",
    "\n",
    "    # Add the entire model directory to the artifact\n",
    "    model_artifact.add_dir(model_dir)\n",
    "\n",
    "    # Perform model evaluation\n",
    "    metrics_summary_df, dists_df, visualizations = eval_fn(**eval_args)\n",
    "\n",
    "    # Save evaluation metrics as artifacts\n",
    "    metrics_summary_csv_path = model_dir / \"metrics_summary.csv\"\n",
    "    metrics_summary_df.to_csv(metrics_summary_csv_path, index=False)\n",
    "    model_artifact.add_file(metrics_summary_csv_path)\n",
    "\n",
    "    dists_csv_path = model_dir / \"detailed_distances.csv\"\n",
    "    dists_df.to_csv(dists_csv_path, index=False)\n",
    "    model_artifact.add_file(dists_csv_path)\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    for metric_name, metric_value in metrics_summary_df.iloc[0].items():\n",
    "        run.summary[metric_name] = metric_value\n",
    "        model_artifact.metadata[metric_name] = metric_value\n",
    "\n",
    "    # Log visualizations\n",
    "    for viz_name, viz_path in visualizations.items():\n",
    "        model_artifact.add_file(viz_path)\n",
    "\n",
    "    # Log the artifact to the W&B run\n",
    "    run.log_artifact(model_artifact, type=\"model\", tags=model_tags)\n",
    "    print(f\"Model artifact '{model_artifact.name}' logged to W&B with evaluations.\")\n",
    "\n",
    "\n",
    "def process_training(project_name, entity_name, experiment_name, version, group, use_existing_model, sleap_train_command, tags=None, model_tags=None):\n",
    "    \"\"\"Processes a training run for a specific version.\n",
    "\n",
    "    Args:\n",
    "        project_name (str): Name of the W&B project--group of experiments.\n",
    "        entity_name (str): Name of the W&B entity--organization or user.\n",
    "        experiment_name (str): Name of the experiment group.\n",
    "        version (str): Version of the training run.\n",
    "        group (pandas.DataFrame): Group of rows corresponding to the version.\n",
    "        use_existing_model (bool): Whether to use an existing model for evaluation.\n",
    "        sleap_train_command (str): Training command to be executed.\n",
    "        tags (list, optional): List of tags to be added to the W&B run.\n",
    "        model_tags (list, optional): List of tags to be added to the model artifact.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    dir_path = Path(group.iloc[0][\"path\"]).parent\n",
    "    print(f\"Directory path for version {version}: {dir_path}\")\n",
    "\n",
    "    config_path = dir_path / f\"initial_config_modified_v00{version}.json\"\n",
    "\n",
    "    if config_path.exists():\n",
    "        # Load the training configuration\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        # Start W&B run\n",
    "        run = log_to_wandb(\n",
    "            project_name=project_name,\n",
    "            entity_name=entity_name,\n",
    "            experiment_name=experiment_name,\n",
    "            version=version,\n",
    "            config=config,\n",
    "            config_path=config_path,\n",
    "            tags=tags\n",
    "        )\n",
    "        if use_existing_model:\n",
    "            # Check if the model directory exists\n",
    "            model_dir = dir_path / \"models\"\n",
    "            if model_dir.exists() and model_dir.is_dir():\n",
    "                # Get all subdirectories in the models directory\n",
    "                subdirectories = [subdir for subdir in model_dir.iterdir() if subdir.is_dir()]\n",
    "\n",
    "                if len(subdirectories) == 1:\n",
    "                    # If exactly one subdirectory exists, get its name\n",
    "                    subdirectory_name = subdirectories[0].name\n",
    "                    print(f\"Subdirectory name: {subdirectory_name}\")\n",
    "\n",
    "                    # Construct the path to the model directory for this run\n",
    "                    model_dir = model_dir / subdirectory_name\n",
    "                    print(f\"Model directory path: {model_dir}\")\n",
    "\n",
    "                    # Log the model as a W&B artifact with evaluation metrics and visualizations\n",
    "                    log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, evaluate_model_and_generate_visuals, {\"model_dir\": model_dir, \"px_per_mm\": 17.0})\n",
    "\n",
    "                elif len(subdirectories) == 0:\n",
    "                    raise FileNotFoundError(f\"No subdirectories found in the models directory for version {version}: {model_dir}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"More than one subdirectory found in the models directory for version {version}: {subdirectories}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Models directory does not exist for version {version}: {model_dir}\")\n",
    "        else:\n",
    "            try:\n",
    "                # Execute training command\n",
    "                command = sleap_train_command.format(config_path.as_posix())\n",
    "                # Debugging: Print the command to verify it is correct\n",
    "                print(f\"Prepared training command: {command}\")\n",
    "                \n",
    "                execute_training(command)\n",
    "\n",
    "                model_dir = dir_path / \"models\"\n",
    "\n",
    "                # Ensure the models directory exists\n",
    "                if model_dir.exists() and model_dir.is_dir():\n",
    "                    # Get all subdirectories in the models directory\n",
    "                    subdirectories = [subdir for subdir in model_dir.iterdir() if subdir.is_dir()]\n",
    "\n",
    "                    if len(subdirectories) == 1:\n",
    "                        # If exactly one subdirectory exists, get its name\n",
    "                        subdirectory_name = subdirectories[0].name\n",
    "                        print(f\"Subdirectory name: {subdirectory_name}\")\n",
    "\n",
    "                        # Construct the path to the model directory for this run\n",
    "                        model_dir = model_dir / subdirectory_name\n",
    "                        print(f\"Model directory path: {model_dir}\")\n",
    "\n",
    "                        # Log the model with evaluation metrics and visualizations as a W&B artifact\n",
    "                        log_model_artifact_with_evals(run, experiment_name, model_tags, model_dir, version, evaluate_model_and_generate_visuals, {\"model_dir\": model_dir, \"px_per_mm\": 17.0})\n",
    "\n",
    "                    elif len(subdirectories) == 0:\n",
    "                        raise FileNotFoundError(f\"No subdirectories found in the models directory for version {version}: {model_dir}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"More than one subdirectory found in the models directory for version {version}: {subdirectories}\")\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Models directory does not exist for version {version}: {model_dir}\")\n",
    "\n",
    "            finally:\n",
    "                # Ensure the W&B run is finished\n",
    "                run.finish()\n",
    "                print(f\"W&B run for version {version} finished.\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Config file not found for version {version}: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(csv_path, use_existing_model=False):\n",
    "    \"\"\"Main function to process all training runs.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (Path): Path to the CSV file containing train-test splits paths.\n",
    "        use_existing_model (bool, optional): Whether to use an existing model for evaluation.\n",
    "    \"\"\"\n",
    "    df = load_training_data(csv_path)\n",
    "    grouped = get_training_groups(df)\n",
    "\n",
    "    for version, group in grouped:\n",
    "        print(f\"Processing version {version}...\")\n",
    "        print(f\"Group: {group}\")\n",
    "        process_training(\n",
    "            project_name=PROJECT_NAME,\n",
    "            entity_name=ENTITY_NAME,\n",
    "            experiment_name=EXPERIMENT_NAME,\n",
    "            version=version,\n",
    "            group=group,\n",
    "            use_existing_model=use_existing_model,\n",
    "            sleap_train_command=SLEAP_TRAIN_COMMAND,\n",
    "            tags=TAGS,\n",
    "            model_tags=MODEL_TAGS\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing version 0...\n",
      "Group:                                                 path  version  labeled_frames  \\\n",
      "0  D:\\SLEAP\\20250102_generalizability_experiment\\...        0             301   \n",
      "1  D:\\SLEAP\\20250102_generalizability_experiment\\...        0              65   \n",
      "2  D:\\SLEAP\\20250102_generalizability_experiment\\...        0              64   \n",
      "\n",
      "  split_type  \n",
      "0      train  \n",
      "1        val  \n",
      "2       test  \n",
      "Directory path for version 0: D:\\SLEAP\\20250102_generalizability_experiment\\lateral\\canola\\train_test_split.v000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meberrigan\u001b[0m (\u001b[33meberrigan-salk-institute-for-biological-studies\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>e:\\repositories\\sleap-roots-training\\wandb\\run-20250121_113955-idjm7e0z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots/runs/idjm7e0z' target=\"_blank\">canola-lateral-2025-01-21_training_v000</a></strong> to <a href='https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots' target=\"_blank\">https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots/runs/idjm7e0z' target=\"_blank\">https://wandb.ai/eberrigan-salk-institute-for-biological-studies/sleap-roots/runs/idjm7e0z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared training command: sleap-train D:/SLEAP/20250102_generalizability_experiment/lateral/canola/train_test_split.v000/initial_config_modified_v000.json\n",
      "Executing: sleap-train D:/SLEAP/20250102_generalizability_experiment/lateral/canola/train_test_split.v000/initial_config_modified_v000.json\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "main(CSV_PATH, use_existing_model=USE_EXISTING_MODEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sleap_v1.3.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
